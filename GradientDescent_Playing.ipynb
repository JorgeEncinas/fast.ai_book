{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "GradientDescent_Playing",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent - Understanding Theory through Play\n",
        "\n",
        "Having completed DeepLearning.AI's \"Deep Learning Specialization\", quite a few of these principles have already stuck with me. Nonetheless, I figured I should stick to the book and keep a rhythm.\n",
        "\n",
        "Let us see then, through interaction, the theory of Gradient Descent"
      ],
      "metadata": {
        "id": "XC06CZPjCuYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai --upgrade -q\n",
        "!pip install fastbook -q"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:04.230753Z",
          "iopub.execute_input": "2024-08-30T05:18:04.231185Z",
          "iopub.status.idle": "2024-08-30T05:18:43.03444Z",
          "shell.execute_reply.started": "2024-08-30T05:18:04.231144Z",
          "shell.execute_reply": "2024-08-30T05:18:43.03288Z"
        },
        "trusted": true,
        "id": "uVOodIV2CuYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from ipywidgets import interact\n",
        "from fastai import *\n",
        "from fastai.basics import *\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "\n",
        "def plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n",
        "    x = torch.linspace(min,max, 100)[:,None]\n",
        "    if ylim: plt.ylim(ylim)\n",
        "    plt.plot(x, f(x), color)\n",
        "    if title is not None: plt.title(title)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:43.037428Z",
          "iopub.execute_input": "2024-08-30T05:18:43.037971Z",
          "iopub.status.idle": "2024-08-30T05:18:50.612022Z",
          "shell.execute_reply.started": "2024-08-30T05:18:43.037911Z",
          "shell.execute_reply": "2024-08-30T05:18:50.610598Z"
        },
        "trusted": true,
        "id": "b_ckjADuCuYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting a function\n",
        "Let's start with defining a function that is quadratic, and seeing a way of fixing different parameters to it (much like model weights). We can find a combination that best fits the data, though it won't match perfectly."
      ],
      "metadata": {
        "id": "luNOj7J5CuYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x): return 3*x**2 + 2*x + 1\n",
        "\n",
        "plot_function(f, \"$3x^2 + 2x + 1$\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:50.613703Z",
          "iopub.execute_input": "2024-08-30T05:18:50.614404Z",
          "iopub.status.idle": "2024-08-30T05:18:51.181631Z",
          "shell.execute_reply.started": "2024-08-30T05:18:50.614358Z",
          "shell.execute_reply": "2024-08-30T05:18:51.180282Z"
        },
        "trusted": true,
        "id": "8ss_ojjHCuYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Partial\n",
        "We create this way of making functions with fixed parameters."
      ],
      "metadata": {
        "id": "ZesAOIR1CuYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quad(a,b,c,x): return a*x**2 + b*x + c"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:51.184586Z",
          "iopub.execute_input": "2024-08-30T05:18:51.185Z",
          "iopub.status.idle": "2024-08-30T05:18:51.191317Z",
          "shell.execute_reply.started": "2024-08-30T05:18:51.184956Z",
          "shell.execute_reply": "2024-08-30T05:18:51.190096Z"
        },
        "trusted": true,
        "id": "QOFjPABtCuYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "def mk_quad(a,b,c): return partial(quad, a, b, c) #This basically creates a function with fixed inputs (parameters)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:51.19309Z",
          "iopub.execute_input": "2024-08-30T05:18:51.193496Z",
          "iopub.status.idle": "2024-08-30T05:18:51.208137Z",
          "shell.execute_reply.started": "2024-08-30T05:18:51.193456Z",
          "shell.execute_reply": "2024-08-30T05:18:51.206861Z"
        },
        "trusted": true,
        "id": "3_xuFK6LCuYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = mk_quad(3,2,1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:51.209645Z",
          "iopub.execute_input": "2024-08-30T05:18:51.210004Z",
          "iopub.status.idle": "2024-08-30T05:18:51.226039Z",
          "shell.execute_reply.started": "2024-08-30T05:18:51.209965Z",
          "shell.execute_reply": "2024-08-30T05:18:51.224813Z"
        },
        "trusted": true,
        "id": "Ucqox2ipCuYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making some noise and sticking to it\n",
        "Let's create a noisy sample by grabbing the original function, which is the optimal curve, and let's add some noise to the entries at random. Thus we know that we want a curve that best fits these points.\n",
        "\n",
        "Now let me clarify what this \"noise\" function is doing, and what the \"add_noise\" function is doing. Do note I asked GPT for some clarity on this, and this text is mostly from it. I'm writing it not to come from a place of seeming like I know, but from a place where I want to internalize these ideas and be able to consult them again later.\n",
        "\n",
        "\"noise(x, scale)\"\n",
        "  - x is the array of the points of the original function, with the true parameters.\n",
        "      We'll use x.shape so that normal(scale=scale, size=x.shape) returns an array of random values with this same shape.\n",
        "  - scale is a bit harder to explain. normal() means these values come from a Normal Distribution. This curve is centered around a mean value that is 0 by default, and then it spreads out according to the Standard Deviation. The scale specified is the Standard Deviation.\n",
        "\n",
        "\n",
        "Do note that the values are not between 0 and 1. Instead they are spread around the mean (which is 0 by default). By default then, most of the generated values will lie within the range of [-3, 3] approximately.\n",
        "When it comes to scale, choosing a smaller one, say, 0.1, makes the values to lie within a smaller range, from about[0.7, 1.3] approximately.\n",
        "\n",
        "\n",
        "Adding 1 to the noise (in add_noise) shifts the mean from 0 to 1, since any value generated now has 1 +- something.\n",
        "\n",
        "To summarize:\n",
        "The normal distribution has a mean of 0 and values are spread out according to scale. A lower scale makes values oscillate closer to the mean (and a larger scale makes them spread out more).\n",
        "\n",
        "And adding to the final value given by normal \"moves\" the mean from 0 to somewhere else.\n",
        "\n",
        "But why is the mean being moved to 1?\n",
        "That is because, if you have values that are oscillating around 0, then on many cases you'll get negative, or close to 0 values. Which as a scale, it might just make many of your values smaller. On the other hand, having a mean of 1, some values will be higher, like 1.3, which make your multiplied value bigger; some values will be smaller, multiplied by something like 0.6"
      ],
      "metadata": {
        "id": "XHN8IyoGCuYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import normal, seed, uniform\n",
        "np.random.seed(42)\n",
        "def noise(x, scale):\n",
        "    return normal(scale=scale, size=x.shape)\n",
        "def add_noise(x, mult, add):\n",
        "    return x * (1+noise(x,mult)) + noise(x,add)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:51.227706Z",
          "iopub.execute_input": "2024-08-30T05:18:51.228796Z",
          "iopub.status.idle": "2024-08-30T05:18:51.240093Z",
          "shell.execute_reply.started": "2024-08-30T05:18:51.228737Z",
          "shell.execute_reply": "2024-08-30T05:18:51.238868Z"
        },
        "trusted": true,
        "id": "x_Wtuaw1CuYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use these functions to graph some points that more or less follow the curve. The noise changes it."
      ],
      "metadata": {
        "id": "k2LG5DECCuYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(-2, 2, steps=20)[:,None] #This gives us 20 evenly-spaced points from -2 to 2\n",
        "y = add_noise(f(x), 0.3, 1.5)\n",
        "#This is adding noise to a function f(x),\n",
        "#with a Multiplication scale of 1.3, and an additive scale of 1.5\n",
        "plt.scatter(x, y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:21:36.109758Z",
          "iopub.execute_input": "2024-08-30T05:21:36.11024Z",
          "iopub.status.idle": "2024-08-30T05:21:36.454167Z",
          "shell.execute_reply.started": "2024-08-30T05:21:36.110182Z",
          "shell.execute_reply": "2024-08-30T05:21:36.452852Z"
        },
        "trusted": true,
        "id": "buYPmmVPCuYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can make out the curve at plain sight, yeah?\n",
        "Now let's create an interactable grapher to try and make a best fit by hand.\n",
        "this is kind of what the ML model does, just in an optimized manner.\n",
        "It's like you could make out which movements are effective towards your goal: which parameters to move in which direction, if only slightly each time.\n",
        "You move a little, see what happened, correct, or keep going."
      ],
      "metadata": {
        "id": "lx88AFxLCuYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#It is here I notice there's a battle between 2020 and 2022 versions of the book."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T03:22:32.354013Z",
          "iopub.execute_input": "2024-08-30T03:22:32.354547Z",
          "iopub.status.idle": "2024-08-30T03:22:32.360378Z",
          "shell.execute_reply.started": "2024-08-30T03:22:32.354498Z",
          "shell.execute_reply": "2024-08-30T03:22:32.358771Z"
        },
        "trusted": true,
        "id": "ftcnr62sCuYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_function??"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T03:22:48.144954Z",
          "iopub.execute_input": "2024-08-30T03:22:48.145535Z",
          "iopub.status.idle": "2024-08-30T03:22:48.162289Z",
          "shell.execute_reply.started": "2024-08-30T03:22:48.145476Z",
          "shell.execute_reply": "2024-08-30T03:22:48.160559Z"
        },
        "trusted": true,
        "id": "BNx_-FSJCuYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact #In my case it was about 3.90, 2.10, 0.20 but it varies bc noise.\n",
        "@interact(a=1.5, b=1.5, c=1.5)\n",
        "def plot_quad(a,b,c):\n",
        "    plt.scatter(x,y)\n",
        "    plot_function(mk_quad(a,b,c), ylim=(-3,12))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T03:21:57.45663Z",
          "iopub.execute_input": "2024-08-30T03:21:57.457136Z",
          "iopub.status.idle": "2024-08-30T03:21:57.801539Z",
          "shell.execute_reply.started": "2024-08-30T03:21:57.457091Z",
          "shell.execute_reply": "2024-08-30T03:21:57.799277Z"
        },
        "trusted": true,
        "id": "TuoTnKv4CuYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectively optimizing, and automatizing objective optimization\n",
        "So we use Loss Functions to optimize our neural network parameters.\n",
        "A neural network is a set of parameters (many, many parameters) to try and best fit a complex non-linear function.\n",
        "\n",
        "This first function is Mean Squared Error (MSE).\n",
        "- preds: the predictions you've made.\n",
        "- acts: the actual values. You must know these to have a loss.\n",
        "\n",
        "Finally, seems like it's the mean of adding every single squared error."
      ],
      "metadata": {
        "id": "RiQ0DP0ACuYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(preds, acts): return ((preds-acts)**2).mean()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:51.241714Z",
          "iopub.execute_input": "2024-08-30T05:18:51.242231Z",
          "iopub.status.idle": "2024-08-30T05:18:51.253376Z",
          "shell.execute_reply.started": "2024-08-30T05:18:51.242148Z",
          "shell.execute_reply": "2024-08-30T05:18:51.252273Z"
        },
        "trusted": true,
        "id": "vT4oaZ6jCuYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@interact(a=1.5, b=1.5, c=1.5)\n",
        "def plot_quad(a,b,c):\n",
        "    f = mk_quad(a,b,c)\n",
        "    plt.scatter(x,y)\n",
        "    loss = mse(f(x), y) #Note we're now using a loss function to objectively see when we play, if we're doing better\n",
        "    plot_function(mk_quad(a,b,c), ylim=(-3,12), title=f\"MSE: {loss:.2f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T03:30:04.09544Z",
          "iopub.execute_input": "2024-08-30T03:30:04.096076Z",
          "iopub.status.idle": "2024-08-30T03:30:04.430751Z",
          "shell.execute_reply.started": "2024-08-30T03:30:04.096015Z",
          "shell.execute_reply": "2024-08-30T03:30:04.429536Z"
        },
        "trusted": true,
        "id": "yBEdKXQWCuYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My guess is that in this extremely low-dimensional function, you can fall in local optima. In high-dimensional problems there are saddle functions everywhere, so the danger of falling into local optima is not that real. It's like many saddle functions. You can get slow cycles of plateauing but you eventually keep moving downwards.\n",
        "\n",
        "Andrew Ng explained that in one of his videos."
      ],
      "metadata": {
        "id": "JFhH8tQQCuYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automate time\n",
        "The function calculates a slope, or a gradient. It's the same thing. It tells you which direction is downwards.\n",
        "You then apply that and move the parameters to a position that lowers the errors: that lowers the loss.\n",
        "\n",
        "So now let's go to actually automate this.\n",
        "\n",
        "1) We need a function that takes the Coefficients of the quadtratic a, b and c as inputs.\n"
      ],
      "metadata": {
        "id": "4PFLPvK6CuYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quad_mse(params):\n",
        "    f = mk_quad(*params) #This spreads the parameters and passes it to the function (destructuring?)\n",
        "    return mse(f(x), y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:18:51.254828Z",
          "iopub.execute_input": "2024-08-30T05:18:51.255187Z",
          "iopub.status.idle": "2024-08-30T05:18:51.266502Z",
          "shell.execute_reply.started": "2024-08-30T05:18:51.25515Z",
          "shell.execute_reply": "2024-08-30T05:18:51.265188Z"
        },
        "trusted": true,
        "id": "Xtk0XHhjCuYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the Tensor it generates. This is the data structure that TensorFlow and PyTorch use (perhaps different tensors, but tensors nonetheless)"
      ],
      "metadata": {
        "id": "QSf8W1Y9CuYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quad_mse([1.5, 1.5, 1.5])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:21:42.131379Z",
          "iopub.execute_input": "2024-08-30T05:21:42.132041Z",
          "iopub.status.idle": "2024-08-30T05:21:42.176612Z",
          "shell.execute_reply.started": "2024-08-30T05:21:42.13198Z",
          "shell.execute_reply": "2024-08-30T05:21:42.175276Z"
        },
        "trusted": true,
        "id": "l9ghWDBKCuYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lists or Vectors of numbers are 1D tensors\n",
        "Rectangles of numbers / Tables are 2D Tensors\n",
        "Layers of tables of numbers are 3D Tensors\n",
        "and so on..."
      ],
      "metadata": {
        "id": "EfwoQ3ZDCuYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In fact, let's put our params into a variable\n",
        "# This is a Rank 1 tensor, which is something Andrew Ng recommends against!\n",
        "abc = torch.tensor([1.5, 1.5, 1.5])\n",
        "\n",
        "# Required grad is flagging this to calculate gradients. In other words, these are weights and\n",
        "# We started the weights at these values, I presume\n",
        "abc.requires_grad_()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:24:22.623216Z",
          "iopub.execute_input": "2024-08-30T05:24:22.624461Z",
          "iopub.status.idle": "2024-08-30T05:24:22.634268Z",
          "shell.execute_reply.started": "2024-08-30T05:24:22.624411Z",
          "shell.execute_reply": "2024-08-30T05:24:22.633048Z"
        },
        "trusted": true,
        "id": "g4FMqyq4CuYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = quad_mse(abc)\n",
        "loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:24:49.705559Z",
          "iopub.execute_input": "2024-08-30T05:24:49.70601Z",
          "iopub.status.idle": "2024-08-30T05:24:49.716124Z",
          "shell.execute_reply.started": "2024-08-30T05:24:49.70596Z",
          "shell.execute_reply": "2024-08-30T05:24:49.71484Z"
        },
        "trusted": true,
        "id": "HZjihwPuCuYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "grad_fn is a function\n",
        "Pytorch knows how to calculate the gradients for our inputs.\n",
        "Let's tell it yes, do that:"
      ],
      "metadata": {
        "id": "RHy3qO2bCuYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:25:50.11094Z",
          "iopub.execute_input": "2024-08-30T05:25:50.111397Z",
          "iopub.status.idle": "2024-08-30T05:25:50.133569Z",
          "shell.execute_reply.started": "2024-08-30T05:25:50.111354Z",
          "shell.execute_reply": "2024-08-30T05:25:50.132405Z"
        },
        "trusted": true,
        "id": "sKIxqX4_CuYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abc.grad"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:25:53.212445Z",
          "iopub.execute_input": "2024-08-30T05:25:53.212887Z",
          "iopub.status.idle": "2024-08-30T05:25:53.224618Z",
          "shell.execute_reply.started": "2024-08-30T05:25:53.212845Z",
          "shell.execute_reply": "2024-08-30T05:25:53.223274Z"
        },
        "trusted": true,
        "id": "TVyr5GCwCuYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is telling us in which direction to move the parameters: add, subtract, subtract.\n",
        "That's right, it's backward. It's like it's telling us the distance in a signed unit, so that we can move it towards 0.\n",
        "\n",
        "Let's apply that"
      ],
      "metadata": {
        "id": "k68m5ffaCuYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    abc -= abc.grad*0.01 #We'll decrease it just one tiny bit, not too much. We may overshoot otherwise.\n",
        "    loss = quad_mse(abc)\n",
        "\n",
        "print(f'loss = {loss:.2f}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:28:13.725712Z",
          "iopub.execute_input": "2024-08-30T05:28:13.72684Z",
          "iopub.status.idle": "2024-08-30T05:28:13.735422Z",
          "shell.execute_reply.started": "2024-08-30T05:28:13.726778Z",
          "shell.execute_reply": "2024-08-30T05:28:13.7341Z"
        },
        "trusted": true,
        "id": "C5R28CIVCuYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch automatically calculates derivatives. But because we're not computing a function,\n",
        "but calculating the gradients instead,\n",
        "we told PyTorch to hold up on the updating, since we're not applying training, just the loss calculation\n",
        "\n",
        "This is the standard grad part of a Neural Network. I actually saw this with tape in TensorFlow.\n",
        "Let's do this many times. This is like iterations in Neural Networks."
      ],
      "metadata": {
        "id": "IuleWp1wCuYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    loss = quad_mse(abc)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        abc -= abc.grad*0.01\n",
        "    print(f\"step={i}; loss={loss:.2f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:32:50.75037Z",
          "iopub.execute_input": "2024-08-30T05:32:50.751542Z",
          "iopub.status.idle": "2024-08-30T05:32:50.76275Z",
          "shell.execute_reply.started": "2024-08-30T05:32:50.751488Z",
          "shell.execute_reply": "2024-08-30T05:32:50.761299Z"
        },
        "trusted": true,
        "id": "xaOVlVB4CuYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see abc\n",
        "abc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:33:27.909129Z",
          "iopub.execute_input": "2024-08-30T05:33:27.909609Z",
          "iopub.status.idle": "2024-08-30T05:33:27.919509Z",
          "shell.execute_reply.started": "2024-08-30T05:33:27.909564Z",
          "shell.execute_reply": "2024-08-30T05:33:27.918271Z"
        },
        "trusted": true,
        "id": "ICa0rda2CuYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigger Problems require more flexible functions\n",
        "\n",
        "Here we're going into Non-Linearity introduced thanks to Activation Functions\n",
        "\n",
        "The most famous one is ReLU: Rectified Linear Unit"
      ],
      "metadata": {
        "id": "4CIHZ2YcCuYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rectified_linear(m,b,x):\n",
        "    y = m*x + b\n",
        "    return torch.clip(y, 0.)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:43:19.378598Z",
          "iopub.execute_input": "2024-08-30T05:43:19.380048Z",
          "iopub.status.idle": "2024-08-30T05:43:19.390855Z",
          "shell.execute_reply.started": "2024-08-30T05:43:19.379981Z",
          "shell.execute_reply": "2024-08-30T05:43:19.389597Z"
        },
        "trusted": true,
        "id": "f2YfsLcyCuYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see they've changed. Now the movement is different!"
      ],
      "metadata": {
        "id": "AOesKi6HCuYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use Partial to create a function and affix b=1, x=1. This is plotting just the base look of ReLU for us to see!"
      ],
      "metadata": {
        "id": "yu99cyMiCuYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_function(partial(rectified_linear, 1, 1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:44:05.074316Z",
          "iopub.execute_input": "2024-08-30T05:44:05.075226Z",
          "iopub.status.idle": "2024-08-30T05:44:05.442304Z",
          "shell.execute_reply.started": "2024-08-30T05:44:05.075146Z",
          "shell.execute_reply": "2024-08-30T05:44:05.441099Z"
        },
        "trusted": true,
        "id": "uOtT0HJgCuYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's play\n",
        "\n",
        "@interact(m=1.5, b=1.5)\n",
        "def plot_relu(m, b):\n",
        "    plot_function(partial(rectified_linear, m, b), ylim=(-1,4))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:46:32.116661Z",
          "iopub.execute_input": "2024-08-30T05:46:32.117127Z",
          "iopub.status.idle": "2024-08-30T05:46:32.399524Z",
          "shell.execute_reply.started": "2024-08-30T05:46:32.117085Z",
          "shell.execute_reply": "2024-08-30T05:46:32.39829Z"
        },
        "trusted": true,
        "id": "zl8ItUBLCuYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a double ReLU. We're visualizing the magic of neurons together."
      ],
      "metadata": {
        "id": "KMaOo-WvCuYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def double_relu(m1, b1, m2, b2, x):\n",
        "    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:49:01.675346Z",
          "iopub.execute_input": "2024-08-30T05:49:01.675902Z",
          "iopub.status.idle": "2024-08-30T05:49:01.683491Z",
          "shell.execute_reply.started": "2024-08-30T05:49:01.675847Z",
          "shell.execute_reply": "2024-08-30T05:49:01.68203Z"
        },
        "trusted": true,
        "id": "Gu-l-U9ZCuY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\n",
        "def plot_double_relu(m1, b1, m2, b2):\n",
        "    plot_function(partial(double_relu, m1, b1, m2, b2), ylim=(-1,6))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-30T05:49:03.881442Z",
          "iopub.execute_input": "2024-08-30T05:49:03.881998Z",
          "iopub.status.idle": "2024-08-30T05:49:04.182236Z",
          "shell.execute_reply.started": "2024-08-30T05:49:03.881944Z",
          "shell.execute_reply": "2024-08-30T05:49:04.180855Z"
        },
        "trusted": true,
        "id": "Mlfw7k7xCuY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see how these together can make more complex functions, and fit different forms of the data."
      ],
      "metadata": {
        "id": "Ffs3XHyOCuY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tips from the lecture\n",
        "He remarks that there are things that are better to know at the start, than at the end.\n",
        "One of them, very important, is that you want to know if it's possible or not.\n",
        "\n",
        "This is like Andrew Ng's approach, in which you iterate continuously in your model.\n",
        "Professor Howard also says this, because if you get something quite accurate, or somewhat accurate, perhaps you CAN work on it. If you can't get any accuracy at all despite what you have, perhaps it's not possible.\n",
        "It's better to find that out first.\n",
        "\n",
        "Iterating is all about trying and taking next steps from there.\n",
        "\n",
        "#### Matrix Multiplication\n",
        "GPUs are basically really good at this computation. It's the foundation of Neural Networks.\n",
        "\n",
        "### Normalization\n",
        "Professor Howard changes a numerical continuous value to go from 0-1 to match some Binary Categorical inputs.\n",
        "\n",
        "### Big and small numbers\n",
        "When this happens, you can take the Log of something. That gets you a lot more even distribution.\n",
        "\n",
        "He mentions fastai is doing all of this for you"
      ],
      "metadata": {
        "id": "ZE6cmjX6CuY0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6poM1SXCuY1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}